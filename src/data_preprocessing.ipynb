{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Library Imports"
      ],
      "metadata": {
        "id": "oQPNL3bYIX-R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "TEZ7ZW-wIS63"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "import joblib\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Dataset"
      ],
      "metadata": {
        "id": "5Dd5Zgv3Ihu3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "file_path = '/content/drive/MyDrive/AI_ML_datasets/energy_data_set.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "print(f\"Dataset loaded successfully!\")\n",
        "print(f\"Shape: {df.shape}\")\n",
        "print(f\"Columns: {list(df.columns)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZGLjeKxIgCf",
        "outputId": "911b674e-0120-4b7d-f932-acca24086202"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Dataset loaded successfully!\n",
            "Shape: (19735, 29)\n",
            "Columns: ['date', 'Appliances', 'lights', 'T1', 'RH_1', 'T2', 'RH_2', 'T3', 'RH_3', 'T4', 'RH_4', 'T5', 'RH_5', 'T6', 'RH_6', 'T7', 'RH_7', 'T8', 'RH_8', 'T9', 'RH_9', 'T_out', 'Press_mm_hg', 'RH_out', 'Windspeed', 'Visibility', 'Tdewpoint', 'rv1', 'rv2']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Type Conversion"
      ],
      "metadata": {
        "id": "Kd14QtfJItWn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert date column to datetime\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "print(\" Date column converted to datetime\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrcjP0puIqy2",
        "outputId": "cbf769c8-558d-4a77-e89a-870b3cdf5b09"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Date column converted to datetime\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Time-based Feature Engineering"
      ],
      "metadata": {
        "id": "oKeCrhM4Ixif"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract time-based features\n",
        "df['hour'] = df['date'].dt.hour\n",
        "df['minute'] = df['date'].dt.minute\n",
        "df['day'] = df['date'].dt.day\n",
        "df['month'] = df['date'].dt.month\n",
        "df['year'] = df['date'].dt.year\n",
        "\n",
        "# Create NSM (Number of Seconds since Midnight)\n",
        "df['NSM'] = df['date'].dt.hour * 3600 + df['date'].dt.minute * 60 + df['date'].dt.second\n",
        "\n",
        "# Adjust the weekday so that 0 = Sunday, 6 = Saturday\n",
        "df['day_of_week'] = (df['date'].dt.dayofweek + 1) % 7\n",
        "\n",
        "# Create WeekStatus\n",
        "df['WeekStatus'] = df['day_of_week'].apply(lambda x: 'Weekend' if x >= 5 else 'Weekday')\n",
        "\n",
        "print(\"Time-based features created successfully!\")\n",
        "print(f\"New columns: {['hour', 'minute', 'day', 'month', 'year', 'NSM', 'day_of_week', 'WeekStatus']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7j_MbXI9Iz3-",
        "outputId": "d90b06ea-4c22-4f70-e881-1b98d1e6aa4f"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time-based features created successfully!\n",
            "New columns: ['hour', 'minute', 'day', 'month', 'year', 'NSM', 'day_of_week', 'WeekStatus']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Missing Values Analysis and Handling"
      ],
      "metadata": {
        "id": "0xWnzgMjI3S_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== MISSING VALUES ANALYSIS ===\")\n",
        "missing_values = df.isnull().sum()\n",
        "missing_percent = (missing_values / len(df)) * 100\n",
        "\n",
        "missing_df = pd.DataFrame({\n",
        "    'Column': missing_values.index,\n",
        "    'Missing_Count': missing_values.values,\n",
        "    'Missing_Percentage': missing_percent.values\n",
        "})\n",
        "\n",
        "# Show only columns with missing values\n",
        "missing_df_filtered = missing_df[missing_df['Missing_Count'] > 0]\n",
        "\n",
        "if len(missing_df_filtered) > 0:\n",
        "    print(\"Columns with missing values:\")\n",
        "    print(missing_df_filtered)\n",
        "\n",
        "\n",
        "else:\n",
        "    print(\"No missing values found in the dataset!\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aoyrosyRI6fu",
        "outputId": "ed142254-549c-4293-a1cf-52fec66359e3"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== MISSING VALUES ANALYSIS ===\n",
            "No missing values found in the dataset!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Outlier Detection"
      ],
      "metadata": {
        "id": "_tNoJLWeNx83"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== OUTLIER DETECTION AND ANALYSIS ===\")\n",
        "\n",
        "def detect_outliers_iqr(df, column):\n",
        "    \"\"\"Detect outliers using IQR method\"\"\"\n",
        "    Q1 = df[column].quantile(0.25)\n",
        "    Q3 = df[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
        "    return outliers, lower_bound, upper_bound\n",
        "\n",
        "def detect_outliers_zscore(df, column, threshold=3):\n",
        "    \"\"\"Detect outliers using Z-score method\"\"\"\n",
        "    z_scores = np.abs(stats.zscore(df[column]))\n",
        "    outliers = df[z_scores > threshold]\n",
        "    return outliers\n",
        "\n",
        "# Analyze outliers in target variable (Appliances)\n",
        "appliances_outliers_iqr, lower_bound, upper_bound = detect_outliers_iqr(df, 'Appliances')\n",
        "appliances_outliers_zscore = detect_outliers_zscore(df, 'Appliances')\n",
        "\n",
        "print(f\"Appliances outliers (IQR method): {len(appliances_outliers_iqr)} ({len(appliances_outliers_iqr)/len(df)*100:.2f}%)\")\n",
        "print(f\"Appliances outliers (Z-score method): {len(appliances_outliers_zscore)} ({len(appliances_outliers_zscore)/len(df)*100:.2f}%)\")\n",
        "print(f\"IQR bounds: Lower={lower_bound:.2f}, Upper={upper_bound:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guZTSjMNN0vf",
        "outputId": "e8428b9e-65cb-4fc0-b803-1400aa8dab05"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== OUTLIER DETECTION AND ANALYSIS ===\n",
            "Appliances outliers (IQR method): 2138 (10.83%)\n",
            "Appliances outliers (Z-score method): 540 (2.74%)\n",
            "IQR bounds: Lower=-25.00, Upper=175.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Outlier Treatment Strategy"
      ],
      "metadata": {
        "id": "2FUfIuxYI_cW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== OUTLIER TREATMENT STRATEGY ===\")\n",
        "\n",
        "# Analyze impact of outliers\n",
        "print(\"Appliances statistics with outliers:\")\n",
        "print(df['Appliances'].describe())\n",
        "\n",
        "# Remove outliers for comparison\n",
        "df_no_outliers = df[~df.index.isin(appliances_outliers_iqr.index)]\n",
        "print(f\"\\nAppliances statistics without outliers:\")\n",
        "print(df_no_outliers['Appliances'].describe())\n",
        "\n",
        "print(f\"\\nImpact of outlier removal:\")\n",
        "print(f\"Original dataset size: {len(df)}\")\n",
        "print(f\"After outlier removal: {len(df_no_outliers)}\")\n",
        "print(f\"Data lost: {len(df) - len(df_no_outliers)} records ({(len(df) - len(df_no_outliers))/len(df)*100:.2f}%)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEa5YzYIJBYm",
        "outputId": "70cdab24-6352-4d73-f89d-ac41b5998b82"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== OUTLIER TREATMENT STRATEGY ===\n",
            "Appliances statistics with outliers:\n",
            "count    19735.000000\n",
            "mean        97.694958\n",
            "std        102.524891\n",
            "min         10.000000\n",
            "25%         50.000000\n",
            "50%         60.000000\n",
            "75%        100.000000\n",
            "max       1080.000000\n",
            "Name: Appliances, dtype: float64\n",
            "\n",
            "Appliances statistics without outliers:\n",
            "count    17597.000000\n",
            "mean        67.209752\n",
            "std         28.480499\n",
            "min         10.000000\n",
            "25%         50.000000\n",
            "50%         60.000000\n",
            "75%         80.000000\n",
            "max        170.000000\n",
            "Name: Appliances, dtype: float64\n",
            "\n",
            "Impact of outlier removal:\n",
            "Original dataset size: 19735\n",
            "After outlier removal: 17597\n",
            "Data lost: 2138 records (10.83%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Keep outliers but cap extreme values"
      ],
      "metadata": {
        "id": "rkrnhFDmKucN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cap_outliers(series, lower_percentile=1, upper_percentile=99):\n",
        "    \"\"\"Cap outliers at specified percentiles\"\"\"\n",
        "    lower_cap = series.quantile(lower_percentile/100)\n",
        "    upper_cap = series.quantile(upper_percentile/100)\n",
        "    return series.clip(lower=lower_cap, upper=upper_cap)\n",
        "\n",
        "# Apply capping to Appliances\n",
        "df['Appliances_capped'] = cap_outliers(df['Appliances'])\n",
        "\n",
        "print(f\"\\n Outlier treatment applied: Capping at 1st and 99th percentiles\")\n",
        "print(f\"Capped statistics:\")\n",
        "print(df['Appliances_capped'].describe())\n",
        "\n",
        "# Use capped version as target\n",
        "df['Appliances'] = df['Appliances_capped']\n",
        "df.drop('Appliances_capped', axis=1, inplace=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpBpXAjsKquh",
        "outputId": "0fdc7d40-409f-492e-bcde-eeb844861719"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Outlier treatment applied: Capping at 1st and 99th percentiles\n",
            "Capped statistics:\n",
            "count    19735.000000\n",
            "mean        96.698090\n",
            "std         96.824925\n",
            "min         20.000000\n",
            "25%         50.000000\n",
            "50%         60.000000\n",
            "75%        100.000000\n",
            "max        576.600000\n",
            "Name: Appliances_capped, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We decided to cap outliers at the 1st and 99th percentiles to balance between outlier impact mitigation and data preservation. This ensures:\n",
        "No data loss (unlike removal),\n",
        "No artificial bias (unlike imputation),\n",
        "More stable and generalizable model training."
      ],
      "metadata": {
        "id": "OnZ7OXzLMFk1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Scaling and Normalization"
      ],
      "metadata": {
        "id": "Q9O7bU7RK-nA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== DATA SCALING AND NORMALIZATION ===\")\n",
        "\n",
        "# Identify numeric columns for scaling\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "# Remove non-feature columns\n",
        "exclude_cols = ['Appliances', 'NSM', 'day_of_week', 'hour', 'minute', 'day', 'month', 'year']\n",
        "feature_cols = [col for col in numeric_cols if col not in exclude_cols]\n",
        "\n",
        "print(f\"Columns to be scaled: {feature_cols}\")\n",
        "\n",
        "# Apply chosen scaling method to all features\n",
        "print(\"=== APPLYING SCALING TO DATASET ===\")\n",
        "\n",
        "# Create a copy for scaling\n",
        "df_scaled = df.copy()\n",
        "\n",
        "# Initialize scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Scale feature columns\n",
        "df_scaled[feature_cols] = scaler.fit_transform(df[feature_cols])\n",
        "\n",
        "print(\" Scaling applied successfully!\")\n",
        "print(f\"Scaled features: {len(feature_cols)} columns\")\n",
        "\n",
        "# Verify scaling worked\n",
        "print(\"\\nBefore scaling (first 5 features):\")\n",
        "print(df[feature_cols[:5]].describe())\n",
        "\n",
        "print(\"\\nAfter scaling (first 5 features):\")\n",
        "print(df_scaled[feature_cols[:5]].describe())\n",
        "\n",
        "# Save scaler for later use\n",
        "joblib.dump(scaler, 'feature_scaler.pkl')\n",
        "print(\" Scaler saved as 'feature_scaler.pkl'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lv47d9JKLDZn",
        "outputId": "4470db14-375b-4632-885c-4a2bdd158b0c"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== DATA SCALING AND NORMALIZATION ===\n",
            "Columns to be scaled: ['lights', 'T1', 'RH_1', 'T2', 'RH_2', 'T3', 'RH_3', 'T4', 'RH_4', 'T5', 'RH_5', 'T6', 'RH_6', 'T7', 'RH_7', 'T8', 'RH_8', 'T9', 'RH_9', 'T_out', 'Press_mm_hg', 'RH_out', 'Windspeed', 'Visibility', 'Tdewpoint', 'rv1', 'rv2']\n",
            "=== APPLYING SCALING TO DATASET ===\n",
            " Scaling applied successfully!\n",
            "Scaled features: 27 columns\n",
            "\n",
            "Before scaling (first 5 features):\n",
            "             lights            T1          RH_1            T2          RH_2\n",
            "count  19735.000000  19735.000000  19735.000000  19735.000000  19735.000000\n",
            "mean       3.801875     21.686571     40.259739     20.341219     40.420420\n",
            "std        7.935988      1.606066      3.979299      2.192974      4.069813\n",
            "min        0.000000     16.790000     27.023333     16.100000     20.463333\n",
            "25%        0.000000     20.760000     37.333333     18.790000     37.900000\n",
            "50%        0.000000     21.600000     39.656667     20.000000     40.500000\n",
            "75%        0.000000     22.600000     43.066667     21.500000     43.260000\n",
            "max       70.000000     26.260000     63.360000     29.856667     56.026667\n",
            "\n",
            "After scaling (first 5 features):\n",
            "             lights            T1          RH_1            T2          RH_2\n",
            "count  19735.000000  1.973500e+04  1.973500e+04  1.973500e+04  1.973500e+04\n",
            "mean       0.000000  3.410317e-15 -9.217073e-17 -2.765122e-16  9.159467e-16\n",
            "std        1.000025  1.000025e+00  1.000025e+00  1.000025e+00  1.000025e+00\n",
            "min       -0.479080 -3.048876e+00 -3.326400e+00 -1.934053e+00 -4.903811e+00\n",
            "25%       -0.479080 -5.769346e-01 -7.354261e-01 -7.073769e-01 -6.193121e-01\n",
            "50%       -0.479080 -5.390414e-02 -1.515563e-01 -1.556007e-01  1.955412e-02\n",
            "75%       -0.479080  5.687512e-01  7.054003e-01  5.284195e-01  6.977352e-01\n",
            "max        8.341722  2.847670e+00  5.805255e+00  4.339172e+00  3.834732e+00\n",
            " Scaler saved as 'feature_scaler.pkl'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Temporal Data Splitting"
      ],
      "metadata": {
        "id": "dQ6lr8q_MooT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== TEMPORAL DATA SPLITTING ===\")\n",
        "\n",
        "# Sort by date to ensure temporal order\n",
        "df_scaled = df_scaled.sort_values('date').reset_index(drop=True)\n",
        "\n",
        "print(f\"Dataset date range:\")\n",
        "print(f\"Start: {df_scaled['date'].min()}\")\n",
        "print(f\"End: {df_scaled['date'].max()}\")\n",
        "print(f\"Total records: {len(df_scaled)}\")\n",
        "\n",
        "# Calculate split point (80% for training, 20% for testing)\n",
        "split_point = int(len(df_scaled) * 0.8)\n",
        "\n",
        "# Split data temporally (NO RANDOM SHUFFLING!)\n",
        "train_data = df_scaled[:split_point].copy()\n",
        "test_data = df_scaled[split_point:].copy()\n",
        "\n",
        "print(f\"\\nTraining set:\")\n",
        "print(f\"  Size: {len(train_data)} records\")\n",
        "print(f\"  Date range: {train_data['date'].min()} to {train_data['date'].max()}\")\n",
        "\n",
        "print(f\"\\nTest set:\")\n",
        "print(f\"  Size: {len(test_data)} records\")\n",
        "print(f\"  Date range: {test_data['date'].min()} to {test_data['date'].max()}\")\n",
        "\n",
        "# Prepare feature matrices and target vectors\n",
        "X_train = train_data[feature_cols]\n",
        "y_train = train_data['Appliances']\n",
        "X_test = test_data[feature_cols]\n",
        "y_test = test_data['Appliances']\n",
        "\n",
        "print(f\"\\nFeature matrix shapes:\")\n",
        "print(f\"X_train: {X_train.shape}\")\n",
        "print(f\"X_test: {X_test.shape}\")\n",
        "print(f\"y_train: {y_train.shape}\")\n",
        "print(f\"y_test: {y_test.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4cymngiMtij",
        "outputId": "11a71316-c310-41b1-8ceb-b4011272a48f"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== TEMPORAL DATA SPLITTING ===\n",
            "Dataset date range:\n",
            "Start: 2016-01-11 17:00:00\n",
            "End: 2016-05-27 18:00:00\n",
            "Total records: 19735\n",
            "\n",
            "Training set:\n",
            "  Size: 15788 records\n",
            "  Date range: 2016-01-11 17:00:00 to 2016-04-30 08:10:00\n",
            "\n",
            "Test set:\n",
            "  Size: 3947 records\n",
            "  Date range: 2016-04-30 08:20:00 to 2016-05-27 18:00:00\n",
            "\n",
            "Feature matrix shapes:\n",
            "X_train: (15788, 27)\n",
            "X_test: (3947, 27)\n",
            "y_train: (15788,)\n",
            "y_test: (3947,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Split Validation"
      ],
      "metadata": {
        "id": "d3c4CZEJM6K7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== DATA SPLIT VALIDATION ===\")\n",
        "\n",
        "# Check for data leakage (critical!)\n",
        "print(\"Checking for data leakage...\")\n",
        "train_max_date = train_data['date'].max()\n",
        "test_min_date = test_data['date'].min()\n",
        "\n",
        "if train_max_date < test_min_date:\n",
        "    print(\" NO DATA LEAKAGE: Training data ends before test data begins\")\n",
        "    print(f\"   Training ends: {train_max_date}\")\n",
        "    print(f\"   Testing begins: {test_min_date}\")\n",
        "else:\n",
        "    print(\" DATA LEAKAGE DETECTED!\")\n",
        "    print(\"   This will cause overly optimistic results!\")\n",
        "\n",
        "# Statistical comparison between train and test sets\n",
        "print(f\"\\n=== STATISTICAL COMPARISON ===\")\n",
        "print(\"Target variable (Appliances) statistics:\")\n",
        "print(f\"Training set - Mean: {y_train.mean():.2f}, Std: {y_train.std():.2f}\")\n",
        "print(f\"Test set - Mean: {y_test.mean():.2f}, Std: {y_test.std():.2f}\")\n",
        "\n",
        "# Feature distribution comparison\n",
        "print(f\"\\nFeature distribution comparison (sample features):\")\n",
        "sample_features = ['T1', 'T_out', 'RH_1', 'lights']\n",
        "for feature in sample_features:\n",
        "    train_mean = train_data[feature].mean()\n",
        "    test_mean = test_data[feature].mean()\n",
        "    print(f\"{feature} - Train: {train_mean:.2f}, Test: {test_mean:.2f}, Diff: {abs(train_mean-test_mean):.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DecZtWsM8dR",
        "outputId": "5d5604ba-20cb-4b9d-cdf6-979b56b0fe3b"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== DATA SPLIT VALIDATION ===\n",
            "Checking for data leakage...\n",
            " NO DATA LEAKAGE: Training data ends before test data begins\n",
            "   Training ends: 2016-04-30 08:10:00\n",
            "   Testing begins: 2016-04-30 08:20:00\n",
            "\n",
            "=== STATISTICAL COMPARISON ===\n",
            "Target variable (Appliances) statistics:\n",
            "Training set - Mean: 96.93, Std: 99.05\n",
            "Test set - Mean: 95.79, Std: 87.36\n",
            "\n",
            "Feature distribution comparison (sample features):\n",
            "T1 - Train: -0.33, Test: 1.33, Diff: 1.66\n",
            "T_out - Train: -0.29, Test: 1.17, Diff: 1.46\n",
            "RH_1 - Train: -0.06, Test: 0.22, Diff: 0.28\n",
            "lights - Train: 0.06, Test: -0.26, Diff: 0.32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Correlation Analysis (for preprocessing validation)"
      ],
      "metadata": {
        "id": "-8-819kCNBY5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== FEATURE CORRELATION ANALYSIS ===\")\n",
        "\n",
        "# Calculate correlation matrix for training data only\n",
        "train_corr_matrix = train_data[feature_cols + ['Appliances']].corr()\n",
        "\n",
        "# Top correlations with target variable\n",
        "target_correlations = train_corr_matrix['Appliances'].abs().sort_values(ascending=False)\n",
        "print(\"Top 10 features correlated with Appliances:\")\n",
        "print(target_correlations[1:11])  # Exclude self-correlation\n",
        "\n",
        "# Highly correlated feature pairs (potential multicollinearity)\n",
        "def find_high_correlations(corr_matrix, threshold=0.8):\n",
        "    high_corr_pairs = []\n",
        "    for i in range(len(corr_matrix.columns)):\n",
        "        for j in range(i+1, len(corr_matrix.columns)):\n",
        "            corr_val = abs(corr_matrix.iloc[i, j])\n",
        "            if corr_val > threshold:\n",
        "                high_corr_pairs.append((corr_matrix.columns[i],\n",
        "                                      corr_matrix.columns[j],\n",
        "                                      corr_val))\n",
        "    return high_corr_pairs\n",
        "\n",
        "high_corr = find_high_correlations(train_corr_matrix[feature_cols].corr(), 0.8)\n",
        "print(f\"\\nHighly correlated feature pairs (>0.8):\")\n",
        "for pair in high_corr[:10]:  # Show top 10\n",
        "    print(f\"{pair[0]} - {pair[1]}: {pair[2]:.3f}\")\n",
        "\n",
        "if len(high_corr) > 0:\n",
        "    print(f\"\\n  Found {len(high_corr)} highly correlated pairs - consider feature selection!\")\n",
        "else:\n",
        "    print(\"\\n No highly correlated features found\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-j1mdnZTNBBB",
        "outputId": "55057e8c-df0a-4b98-caea-8d3ded75c9a4"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== FEATURE CORRELATION ANALYSIS ===\n",
            "Top 10 features correlated with Appliances:\n",
            "lights    0.223721\n",
            "T2        0.162093\n",
            "RH_out    0.157957\n",
            "T6        0.128450\n",
            "T3        0.126982\n",
            "RH_8      0.116507\n",
            "T_out     0.112589\n",
            "RH_1      0.096134\n",
            "T1        0.089316\n",
            "RH_6      0.083196\n",
            "Name: Appliances, dtype: float64\n",
            "\n",
            "Highly correlated feature pairs (>0.8):\n",
            "T1 - T2: 0.865\n",
            "T1 - T3: 0.974\n",
            "T1 - T4: 0.966\n",
            "T1 - T5: 0.974\n",
            "T1 - RH_6: 0.890\n",
            "T1 - T7: 0.936\n",
            "T1 - T8: 0.936\n",
            "T1 - T9: 0.946\n",
            "RH_1 - RH_2: 0.928\n",
            "RH_1 - RH_3: 0.954\n",
            "\n",
            "  Found 66 highly correlated pairs - consider feature selection!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing Summary Report"
      ],
      "metadata": {
        "id": "cZ0UlBnINTmB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"           DATA PREPROCESSING SUMMARY REPORT\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\\n DATASET OVERVIEW:\")\n",
        "print(f\"   • Original dataset size: {df.shape[0]:,} records, {df.shape[1]} features\")\n",
        "print(f\"   • Date range: {df['date'].min()} to {df['date'].max()}\")\n",
        "print(f\"   • Sampling interval: 10 minutes\")\n",
        "print(f\"   • Duration: {(df['date'].max() - df['date'].min()).days} days\")\n",
        "\n",
        "print(f\"\\n DATA QUALITY ASSESSMENT:\")\n",
        "missing_count = df.isnull().sum().sum()\n",
        "print(f\"   • Missing values: {missing_count} total\")\n",
        "print(f\"   • Outliers detected: {len(appliances_outliers_iqr)} ({len(appliances_outliers_iqr)/len(df)*100:.1f}%)\")\n",
        "print(f\"   • Outlier treatment: Capping applied\")\n",
        "\n",
        "print(f\"\\n FEATURE ENGINEERING:\")\n",
        "print(f\"   • Time-based features created: hour, day_of_week, WeekStatus, NSM\")\n",
        "print(f\"   • Temperature features: {len([c for c in feature_cols if c.startswith('T')])}\")\n",
        "print(f\"   • Humidity features: {len([c for c in feature_cols if c.startswith('RH')])}\")\n",
        "print(f\"   • Weather features: 3 (Windspeed, Visibility, Press_mm_hg)\")\n",
        "print(f\"   • Total features for modeling: {len(feature_cols)}\")\n",
        "\n",
        "print(f\"\\n DATA SCALING:\")\n",
        "print(f\"   • Method: StandardScaler (z-score normalization)\")\n",
        "print(f\"   • Reason: Different feature scales, suitable for neural networks\")\n",
        "print(f\"   • Features scaled: {len(feature_cols)}\")\n",
        "\n",
        "print(f\"\\n TEMPORAL DATA SPLIT:\")\n",
        "print(f\"   • Training set: {len(X_train):,} records ({len(X_train)/len(df)*100:.1f}%)\")\n",
        "print(f\"   • Test set: {len(X_test):,} records ({len(X_test)/len(df)*100:.1f}%)\")\n",
        "print(f\"   • Split method: Temporal (no shuffling)\")\n",
        "print(f\"   • Data leakage: {' None detected' if train_max_date < test_min_date else ' DETECTED!'}\")\n",
        "\n",
        "print(f\"\\n TARGET VARIABLE (Appliances):\")\n",
        "print(f\"   • Training mean: {y_train.mean():.2f} Wh\")\n",
        "print(f\"   • Training std: {y_train.std():.2f} Wh\")\n",
        "print(f\"   • Range: {y_train.min():.0f} - {y_train.max():.0f} Wh\")\n",
        "\n",
        "print(f\"\\n KEY CORRELATIONS WITH TARGET:\")\n",
        "top_5_corr = target_correlations[1:6]\n",
        "for feature, corr in top_5_corr.items():\n",
        "    print(f\"   • {feature}: {corr:.3f}\")\n",
        "\n",
        "print(f\"\\n PREPROCESSING STATUS: COMPLETE\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYezvYuZNWTY",
        "outputId": "39cdad67-d5e3-4759-96dc-ef60a95db18f"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "           DATA PREPROCESSING SUMMARY REPORT\n",
            "============================================================\n",
            "\n",
            " DATASET OVERVIEW:\n",
            "   • Original dataset size: 19,735 records, 37 features\n",
            "   • Date range: 2016-01-11 17:00:00 to 2016-05-27 18:00:00\n",
            "   • Sampling interval: 10 minutes\n",
            "   • Duration: 137 days\n",
            "\n",
            " DATA QUALITY ASSESSMENT:\n",
            "   • Missing values: 0 total\n",
            "   • Outliers detected: 2138 (10.8%)\n",
            "   • Outlier treatment: Capping applied\n",
            "\n",
            " FEATURE ENGINEERING:\n",
            "   • Time-based features created: hour, day_of_week, WeekStatus, NSM\n",
            "   • Temperature features: 11\n",
            "   • Humidity features: 10\n",
            "   • Weather features: 3 (Windspeed, Visibility, Press_mm_hg)\n",
            "   • Total features for modeling: 27\n",
            "\n",
            " DATA SCALING:\n",
            "   • Method: StandardScaler (z-score normalization)\n",
            "   • Reason: Different feature scales, suitable for neural networks\n",
            "   • Features scaled: 27\n",
            "\n",
            " TEMPORAL DATA SPLIT:\n",
            "   • Training set: 15,788 records (80.0%)\n",
            "   • Test set: 3,947 records (20.0%)\n",
            "   • Split method: Temporal (no shuffling)\n",
            "   • Data leakage:  None detected\n",
            "\n",
            " TARGET VARIABLE (Appliances):\n",
            "   • Training mean: 96.93 Wh\n",
            "   • Training std: 99.05 Wh\n",
            "   • Range: 20 - 577 Wh\n",
            "\n",
            " KEY CORRELATIONS WITH TARGET:\n",
            "   • lights: 0.224\n",
            "   • T2: 0.162\n",
            "   • RH_out: 0.158\n",
            "   • T6: 0.128\n",
            "   • T3: 0.127\n",
            "\n",
            " PREPROCESSING STATUS: COMPLETE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save Preprocessed Data"
      ],
      "metadata": {
        "id": "BEjv4j1INdj4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== SAVING PREPROCESSED DATA ===\")\n",
        "\n",
        "# Save datasets\n",
        "train_data.to_csv('train_data_preprocessed.csv', index=False)\n",
        "test_data.to_csv('test_data_preprocessed.csv', index=False)\n",
        "\n",
        "\n",
        "# Save feature names\n",
        "with open('feature_names.txt', 'w') as f:\n",
        "    for feature in feature_cols:\n",
        "        f.write(f\"{feature}\\n\")\n",
        "\n",
        "# Save preprocessing metadata\n",
        "preprocessing_info = {\n",
        "    'feature_columns': feature_cols,\n",
        "    'scaler_type': 'StandardScaler',\n",
        "    'split_point': split_point,\n",
        "    'train_size': len(X_train),\n",
        "    'test_size': len(X_test),\n",
        "    'outlier_treatment': 'capping_1_99_percentiles',\n",
        "    'date_range_train': [str(train_data['date'].min()), str(train_data['date'].max())],\n",
        "    'date_range_test': [str(test_data['date'].min()), str(test_data['date'].max())]\n",
        "}\n",
        "\n",
        "with open('preprocessing_info.json', 'w') as f:\n",
        "    json.dump(preprocessing_info, f, indent=2)\n",
        "\n",
        "print(\" Saved files:\")\n",
        "print(\"   • train_data_preprocessed.csv\")\n",
        "print(\"   • test_data_preprocessed.csv\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZdO885mNb8I",
        "outputId": "2b5c5d0b-c1d2-4a7e-f09c-944691c7c83d"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== SAVING PREPROCESSED DATA ===\n",
            " Saved files:\n",
            "   • train_data_preprocessed.csv\n",
            "   • test_data_preprocessed.csv\n"
          ]
        }
      ]
    }
  ]
}